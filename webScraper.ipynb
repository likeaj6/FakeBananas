{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using user provided API key for making requests\n",
      "Event Registry host: http://eventregistry.org\n"
     ]
    }
   ],
   "source": [
    "from eventregistry import *\n",
    "from newspaper import Article\n",
    "from threading import Thread, Lock\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# Print a list of recently added articles mentioning entered words\n",
    "api_key = 'eda39267-9017-481a-860d-0b565c6d8bf3'\n",
    "er = EventRegistry(apiKey = api_key)\n",
    "\n",
    "global_df = pd.DataFrame()\n",
    "mutex = Lock()\n",
    "\n",
    "def get_articles(keywords,count=199): \n",
    "    global global_df\n",
    "    q = QueryArticlesIter(keywords=QueryItems.AND(keywords))\n",
    "    q.setRequestedResult(RequestArticlesInfo(count= count, sortBy=\"sourceImportance\"))\n",
    "    print keywords\n",
    "    \n",
    "    x = 0\n",
    "    \n",
    "    local_df = pd.DataFrame()\n",
    "    \n",
    "    res = er.execQuery(q)\n",
    "    for article in res['articles']['results']:\n",
    "        data = {\n",
    "            'source': article['source']['title'].encode('utf-8'),\n",
    "#             'title' : article['title'].encode('utf-8'),\n",
    "            'url' : article['url'].encode('utf-8'),\n",
    "            'text' : article['body'].encode('utf-8')\n",
    "        }\n",
    "        local_df = pd.concat([local_df, pd.DataFrame(data,index=[x])])\n",
    "        x += 1\n",
    "        \n",
    "    mutex.acquire()\n",
    "    try:\n",
    "        global_df = pd.concat([global_df,local_df])\n",
    "    finally:\n",
    "        mutex.release()\n",
    "\n",
    "def get_search_params(keywords):\n",
    "    search_params = []\n",
    "    while len(keywords) != 0:\n",
    "        # Randomly select 3 words\n",
    "        rm = random.sample(keywords,3)\n",
    "        # add the list of 3 words to the searchable list\n",
    "        search_params.append(rm)\n",
    "        # remove words from the list\n",
    "        for word in rm:\n",
    "            keywords.remove(word)\n",
    "\n",
    "        # put 1 or 2 random words back\n",
    "        # if 3 words left just append to search_params\n",
    "        if len(keywords) is 3:\n",
    "            search_params.append(keywords)\n",
    "            keywords = []\n",
    "        # if no words left just exit\n",
    "        elif len(keywords) is 0:\n",
    "            keywords = []\n",
    "        # if 1 word left, append 2 and search_params\n",
    "        elif len(keywords) is 1:\n",
    "            keywords.append(random.sample(rm,2)[0:2])\n",
    "        else:\n",
    "            keywords.append(random.sample(rm,1)[0])\n",
    "    return search_params\n",
    "\n",
    "def get_keywords(user_url):\n",
    "    url = user_url.decode('utf-8')\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "\n",
    "    keywords = article.keywords\n",
    "    kl = []\n",
    "    for word in keywords:\n",
    "        kl.append(word.encode('utf-8'))\n",
    "    return kl\n",
    "\n",
    "def watson(user_url):\n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "      username=\"09b56387-57ee-4390-9365-a07a37706fb4\",\n",
    "      password=\"ISoTe5EueZJp\",\n",
    "      version=\"2017-02-27\")\n",
    "\n",
    "    response = natural_language_understanding.analyze(\n",
    "      url=user_url,\n",
    "      features=[\n",
    "        Features.Keywords(\n",
    "          emotion=False,\n",
    "          sentiment=False,\n",
    "            limit=15\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    keywords = []\n",
    "    for keyword in response['keywords']:\n",
    "        if keyword['relevance'] > 0.80 and len(keywords) < 8:\n",
    "            keywords.append(keyword['text'].encode('utf-8'))\n",
    "    return keywords\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, query):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.query = query\n",
    "\n",
    "    def run(self):\n",
    "        get_articles(self.query)\n",
    "        \n",
    "def web_scrape(url):\n",
    "    global global_df\n",
    "    kl = get_keywords(url)\n",
    "    params = get_search_params(kl)\n",
    "\n",
    "    index = 0\n",
    "    threads = []\n",
    "    \n",
    "    for query in params:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "#     df.to_json(orient='index')\n",
    "    global_df.to_csv('articles.csv')\n",
    "    print global_df\n",
    "    \n",
    "def watson_scrape(url):\n",
    "    global global_df\n",
    "    keywords = watson(url)\n",
    "    \n",
    "    index = 0\n",
    "    threads = []\n",
    "    \n",
    "    for query in keywords:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "    global_df.to_csv('watson_articles.csv')\n",
    "    print global_df\n",
    "    \n",
    "def watson_claim(claim):\n",
    "    natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "      username=\"09b56387-57ee-4390-9365-a07a37706fb4\",\n",
    "      password=\"ISoTe5EueZJp\",\n",
    "      version=\"2017-02-27\")\n",
    "\n",
    "    response = natural_language_understanding.analyze(\n",
    "      url=user_url,\n",
    "      features=[\n",
    "        Features.Keywords(\n",
    "          emotion=False,\n",
    "          sentiment=False,\n",
    "            limit=5\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    keywords = []\n",
    "    for keyword in response['keywords']:\n",
    "        if keyword['relevance'] > 0.80 and len(keywords) < 8:\n",
    "            keywords.append(keyword['text'].encode('utf-8'))\n",
    "    return keywords\n",
    "\n",
    "def watson_scrape_claim(claim):\n",
    "    global global_df\n",
    "    \n",
    "    keywords = watson_claim(claim)\n",
    "    \n",
    "    index = 0\n",
    "    threads = []\n",
    "    \n",
    "    for query in keywords:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "    global_df.to_csv('watson_articles.csv')\n",
    "    print global_df\n",
    "    \n",
    "def main():\n",
    "    url = 'http://abcnews.go.com/US/wireStory/hurricanes-teach-us-ap-finds-fast-coastal-growth-49893843'\n",
    "#     web_scrape(url)\n",
    "#     watson_scrape(url)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global warming']\n",
      "['global warming']\n",
      "['global warming', 'greenhouse gas emissions']\n"
     ]
    }
   ],
   "source": [
    "#To run this you must have installed\n",
    "#####for python 2.7.*\n",
    "#pip install py-ms-cognitive\n",
    "#####for python 3.*\n",
    "#pip3 install py-ms-cognitive\n",
    "\n",
    "from py_ms_cognitive import PyMsCognitiveWebSearch\n",
    "import nltk\n",
    "\n",
    "def azure_search(claim):\n",
    "    search_term = claim\n",
    "    search_service = PyMsCognitiveWebSearch('75d1a40af4bf4ba4bdf561ae25b5db5c', claim)\n",
    "    first_three_result = search_service.search(limit=3, format='json') #1-50\n",
    "\n",
    "    urls = []\n",
    "   # To get individual result json:\n",
    "    for i in first_three_result:\n",
    "        urls.append(i.url.encode('utf-8'))\n",
    "    return urls\n",
    "\n",
    "def azure_claim(urls):\n",
    "    keywords = []\n",
    "    for url in urls:\n",
    "        keywords.append(watson(url))\n",
    "    return keywords\n",
    "    \n",
    "claim = 'Global Warming is not real'\n",
    "claim_urls = azure_search(claim)\n",
    "\n",
    "def watson_azure_scrape(keywords):\n",
    "    global global_df\n",
    "\n",
    "    index = 0\n",
    "    threads = []\n",
    "\n",
    "    for query in keywords:\n",
    "        threads.append(myThread(query))\n",
    "        threads[index].start()\n",
    "        index += 1\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    global_df = global_df.reset_index(drop=True)\n",
    "    global_df.to_csv('watson_articles.csv')\n",
    "#     global_df['uid'] = range(len(global_df.index))\n",
    "#     return global_df.to_dict(orient='records')\n",
    "\n",
    "def run_azure(claim):\n",
    "    claim_tokens = nltk.word_tokenize(claim)\n",
    "    if len(claim_tokens) is 3:\n",
    "        # Go straight to event registry with claim\n",
    "        watson_azure_scrape(claim)\n",
    "    else: \n",
    "        watson_azure_scrape(azure_claim(azure_search(claim)))\n",
    "run_azure(claim)\n",
    "\n",
    "\n",
    "\n",
    "   # To get the whole response json from the MOST RECENT response\n",
    "    # (which will hold 50 individual responses depending on limit set):\n",
    "#print (search_service.most_recent_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['obama', 'Barack Obama'], ['Obama', 'Barack Hussein Obama'], ['Trump']]\n"
     ]
    }
   ],
   "source": [
    "azure_claim(claim_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arrays were different lengths: 1262 vs 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a808763dbe9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Mail Online'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henry\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other, axis)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[1;32mC:\\Users\\henry\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henry\\Anaconda2\\lib\\site-packages\\pandas\\core\\ops.pyc\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.vec_compare (pandas\\_libs\\lib.c:14284)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Arrays were different lengths: 1262 vs 1"
     ]
    }
   ],
   "source": [
    "claim = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  source                                               text  \\\n",
      "0                   test                                            testing   \n",
      "0     The New York Times  FAIR BLUFF, N.C. -- The flooding is long gone,...   \n",
      "1     The New York Times  WASHINGTON -- As Hurricane Irma rampaged throu...   \n",
      "2     The New York Times  CLEWISTON, Fla. -- Vicious hurricanes all in a...   \n",
      "3     The New York Times  The White House says Trump spoke to the offici...   \n",
      "4     The New York Times  But even after the storm illustrated the city'...   \n",
      "5     The New York Times  A hurricane watch was issued for Antigua and B...   \n",
      "6     The New York Times  About 4 a.m. local time, the storm was 40 mile...   \n",
      "7     The New York Times  \"A lot of people died,\" Mr. Marte said of Harv...   \n",
      "8     The New York Times  MEXICO CITY -- Sorry, we have too many trouble...   \n",
      "9     The New York Times  If you're flying, call your airline to get a s...   \n",
      "10    The New York Times  Over those years, many things changed. Sporadi...   \n",
      "11    The New York Times  If you're flying, call your airline to get a s...   \n",
      "12    The New York Times  ST. AUGUSTINE, Fla. -- This place calls itself...   \n",
      "13    The New York Times  JACKSONVILLE, Fla. -- Stressed and exhausted f...   \n",
      "14    The New York Times  Whether policyholders can be made whole may de...   \n",
      "15    The New York Times  FORT MYERS, Fla. -- Engineers stopped pumping ...   \n",
      "16    The New York Times  * The National Hurricane Center downgraded the...   \n",
      "17    The New York Times  NAPLES, Fla. -- Across coastal Florida, the dr...   \n",
      "18    The New York Times  Hurricane Irma's sudden drive to the west prom...   \n",
      "19    The New York Times  But an array of political leaders -- including...   \n",
      "20    The New York Times  The pain was felt where the storm hit hardest,...   \n",
      "21    The New York Times  WASHINGTON -- As Hurricane Irma barrels danger...   \n",
      "22    The New York Times  Re \"For E.P.A. Chief, Discussing Climate After...   \n",
      "23    The New York Times  Watch the Times video below, of Hurricane Irma...   \n",
      "24    The New York Times  We asked Times journalists covering Hurricane ...   \n",
      "25    The New York Times  Throughout Texas and Louisiana, Hurricane Harv...   \n",
      "26    The New York Times  WASHINGTON -- The last time a Category 5 hurri...   \n",
      "27    The New York Times  As Hurricane Irma began churning through the F...   \n",
      "28    The New York Times  And yet: \"I'm going to stay here the rest of m...   \n",
      "..                   ...                                                ...   \n",
      "169          Mail Online  A police checkpoint on US Highway 1 blocks acc...   \n",
      "170             BBC News  Donald Trump has agreed to a Democratic plan t...   \n",
      "171          Mail Online  WASHINGTON, Sept 15 (Reuters) - U.S. retail sa...   \n",
      "172          Mail Online  By Mark Weinraub\\n\\nWASHINGTON, Sept 12 (Reute...   \n",
      "173          Mail Online  All former living presidents have come togethe...   \n",
      "174          Mail Online  A woman buys wine across from the empty water ...   \n",
      "175          Mail Online  This photo gallery highlights some of the top ...   \n",
      "176          Mail Online  Deal or no deal? 'Dreamers' wait as Trump, law...   \n",
      "177          Mail Online  This image obtained from NASA's GOES Project s...   \n",
      "178          Mail Online  SAVANNAH, Ga. (AP) - Shifting forecasts raised...   \n",
      "179          Mail Online  This satellite image obtained from the Nationa...   \n",
      "180             BBC News  The US House of Representatives has approved a...   \n",
      "181          Mail Online  MIAMI (AP) - Hurricane Irma's relentless advan...   \n",
      "182          Mail Online  Jenny Marie Hill and partner Anthony Broadbent...   \n",
      "183          Mail Online  Hurricane Irma has grown into a powerful Categ...   \n",
      "184          Mail Online  By Emily Flitter and Steve Holland\\n\\nHOUSTON,...   \n",
      "185          Mail Online  By Valerie Volcovici and Alister Doyle\\n\\nWASH...   \n",
      "186          Mail Online  FORT LAUDERDALE, Fla. (AP) - They call it the ...   \n",
      "187          Mail Online  As Irma's misery grows in scope and scale, so ...   \n",
      "188          Mail Online  WASHINGTON (AP) - A powerful Hurricane Irma is...   \n",
      "189          Mail Online  CAIBARIEN, Cuba (AP) - Cuba evacuated tourists...   \n",
      "190  The Huffington Post  JACKSONVILLE, Fla. -- Hurricane Irma weakened ...   \n",
      "191          Mail Online  Two people have died in Georgia as a result of...   \n",
      "192          Mail Online  A total of 6.3 million people were told to lea...   \n",
      "193          Mail Online  Labor day festivities won't be spoiled for mos...   \n",
      "194          Mail Online  SAVANNAH, Ga. (AP) - Shifting forecasts raised...   \n",
      "195          Mail Online  Mo the sloth and Kramer the emu are as innocen...   \n",
      "196          Mail Online  WASHINGTON (AP) - Irma, which flattened some C...   \n",
      "197          Mail Online  By Scott Malone\\n\\nSAN JUAN, Puerto Rico, Sept...   \n",
      "198          Mail Online  IMMOKALEE, Fla. (AP) - Millions of poor people...   \n",
      "\n",
      "                                                   url  \n",
      "0                                          placeholers  \n",
      "0    https://www.nytimes.com/2017/09/08/us/north-ca...  \n",
      "1    https://www.nytimes.com/2017/09/12/climate/flo...  \n",
      "2    https://www.nytimes.com/2017/09/08/us/hurrican...  \n",
      "3    https://www.nytimes.com/aponline/2017/09/06/us...  \n",
      "4    https://www.nytimes.com/2017/09/12/briefing/hu...  \n",
      "5    https://www.nytimes.com/2017/09/07/world/ameri...  \n",
      "6    https://www.nytimes.com/2017/09/06/world/ameri...  \n",
      "7    https://www.nytimes.com/2017/09/08/us/irma-har...  \n",
      "8    https://www.nytimes.com/2017/09/12/world/ameri...  \n",
      "9    https://www.nytimes.com/2017/09/06/travel/what...  \n",
      "10   https://www.nytimes.com/2017/09/12/us/hurrican...  \n",
      "11   https://www.nytimes.com/2017/09/06/travel/irma...  \n",
      "12   https://www.nytimes.com/2017/09/13/us/st-augus...  \n",
      "13   https://www.nytimes.com/2017/09/12/us/irma-jac...  \n",
      "14   https://www.nytimes.com/2017/09/11/business/ir...  \n",
      "15   https://www.nytimes.com/2017/09/09/us/irma-flo...  \n",
      "16   https://www.nytimes.com/2017/09/08/us/hurrican...  \n",
      "17   https://www.nytimes.com/2017/09/11/us/storm-su...  \n",
      "18   https://www.nytimes.com/2017/09/10/us/hurrican...  \n",
      "19   https://www.nytimes.com/2017/09/14/us/irma-har...  \n",
      "20   https://www.nytimes.com/2017/09/12/us/irma-har...  \n",
      "21   https://www.nytimes.com/2017/09/08/climate/how...  \n",
      "22   https://www.nytimes.com/2017/09/12/opinion/tru...  \n",
      "23   https://www.nytimes.com/2017/09/14/learning/te...  \n",
      "24   https://www.nytimes.com/2017/09/10/us/hurrican...  \n",
      "25   https://www.nytimes.com/2017/09/14/smarter-liv...  \n",
      "26   https://www.nytimes.com/2017/09/07/climate/flo...  \n",
      "27   https://www.nytimes.com/2017/09/10/us/miami-ke...  \n",
      "28   https://www.nytimes.com/2017/09/12/us/keys-res...  \n",
      "..                                                 ...  \n",
      "169  http://www.dailymail.co.uk/~/article-4874470/i...  \n",
      "170  http://www.bbc.co.uk/news/world-us-canada-4118...  \n",
      "171  http://www.dailymail.co.uk/~/article-4887894/i...  \n",
      "172  http://www.dailymail.co.uk/~/article-4877232/i...  \n",
      "173  http://www.dailymail.co.uk/~/article-4864324/i...  \n",
      "174  http://www.dailymail.co.uk/~/article-4868792/i...  \n",
      "175  http://www.dailymail.co.uk/~/article-4886634/i...  \n",
      "176  http://www.dailymail.co.uk/~/article-4886670/i...  \n",
      "177  http://www.dailymail.co.uk/~/article-4852190/i...  \n",
      "178  http://www.dailymail.co.uk/~/article-4861102/i...  \n",
      "179  http://www.dailymail.co.uk/~/article-4859684/i...  \n",
      "180  http://www.bbc.co.uk/news/world-us-canada-4120...  \n",
      "181  http://www.dailymail.co.uk/~/article-4867516/i...  \n",
      "182  http://www.dailymail.co.uk/~/article-4865410/i...  \n",
      "183  http://www.dailymail.co.uk/~/article-4852228/i...  \n",
      "184  http://www.dailymail.co.uk/~/article-4846894/i...  \n",
      "185  http://www.dailymail.co.uk/~/article-4871652/i...  \n",
      "186  http://www.dailymail.co.uk/~/article-4863406/i...  \n",
      "187  http://www.dailymail.co.uk/~/article-4888362/i...  \n",
      "188  http://www.dailymail.co.uk/~/article-4859600/i...  \n",
      "189  http://www.dailymail.co.uk/~/article-4865540/i...  \n",
      "190  http://www.huffingtonpost.com/entry/jacksonvil...  \n",
      "191  http://www.dailymail.co.uk/~/article-4874380/i...  \n",
      "192  http://www.dailymail.co.uk/~/article-4872424/i...  \n",
      "193  http://www.dailymail.co.uk/~/article-4849828/i...  \n",
      "194  http://www.dailymail.co.uk/~/article-4859948/i...  \n",
      "195  http://www.dailymail.co.uk/~/article-4869592/i...  \n",
      "196  http://www.dailymail.co.uk/~/article-4883232/i...  \n",
      "197  http://www.dailymail.co.uk/~/article-4859972/i...  \n",
      "198  http://www.dailymail.co.uk/~/article-4883210/i...  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts['articles']['results'][0]['source']['title']\n",
    "len(arts['articles']['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('100_articles.csv')\n",
    "df1.columns = ['id','source','text',\"\"\"'title\"\"\" 'url']\n",
    "\n",
    "bodies = df1.loc[:,['id','text']]\n",
    "bodies.to_csv('bodies.csv')\n",
    "\n",
    "claim = [claim] * len(global_df.index)\n",
    "claims = pd.DataFrame(claim)\n",
    "claims['id'] = range(len(global_df.index))\n",
    "claims.to_csv('claims.csv')\n",
    "\n",
    "# we want claim here, not title\n",
    "# headlines = df1.loc[:,['id','title']]\n",
    "# headlines.to_csv('headlines.csv')\n",
    "\n",
    "urls = df1.loc[:,['id','source','url']]\n",
    "urls.to_csv('url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Brazilian junk food</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0   id\n",
       "0    Brazilian junk food    0\n",
       "1    Brazilian junk food    1\n",
       "2    Brazilian junk food    2\n",
       "3    Brazilian junk food    3\n",
       "4    Brazilian junk food    4\n",
       "5    Brazilian junk food    5\n",
       "6    Brazilian junk food    6\n",
       "7    Brazilian junk food    7\n",
       "8    Brazilian junk food    8\n",
       "9    Brazilian junk food    9\n",
       "10   Brazilian junk food   10\n",
       "11   Brazilian junk food   11\n",
       "12   Brazilian junk food   12\n",
       "13   Brazilian junk food   13\n",
       "14   Brazilian junk food   14\n",
       "15   Brazilian junk food   15\n",
       "16   Brazilian junk food   16\n",
       "17   Brazilian junk food   17\n",
       "18   Brazilian junk food   18\n",
       "19   Brazilian junk food   19\n",
       "20   Brazilian junk food   20\n",
       "21   Brazilian junk food   21\n",
       "22   Brazilian junk food   22\n",
       "23   Brazilian junk food   23\n",
       "24   Brazilian junk food   24\n",
       "25   Brazilian junk food   25\n",
       "26   Brazilian junk food   26\n",
       "27   Brazilian junk food   27\n",
       "28   Brazilian junk food   28\n",
       "29   Brazilian junk food   29\n",
       "..                   ...  ...\n",
       "71   Brazilian junk food   71\n",
       "72   Brazilian junk food   72\n",
       "73   Brazilian junk food   73\n",
       "74   Brazilian junk food   74\n",
       "75   Brazilian junk food   75\n",
       "76   Brazilian junk food   76\n",
       "77   Brazilian junk food   77\n",
       "78   Brazilian junk food   78\n",
       "79   Brazilian junk food   79\n",
       "80   Brazilian junk food   80\n",
       "81   Brazilian junk food   81\n",
       "82   Brazilian junk food   82\n",
       "83   Brazilian junk food   83\n",
       "84   Brazilian junk food   84\n",
       "85   Brazilian junk food   85\n",
       "86   Brazilian junk food   86\n",
       "87   Brazilian junk food   87\n",
       "88   Brazilian junk food   88\n",
       "89   Brazilian junk food   89\n",
       "90   Brazilian junk food   90\n",
       "91   Brazilian junk food   91\n",
       "92   Brazilian junk food   92\n",
       "93   Brazilian junk food   93\n",
       "94   Brazilian junk food   94\n",
       "95   Brazilian junk food   95\n",
       "96   Brazilian junk food   96\n",
       "97   Brazilian junk food   97\n",
       "98   Brazilian junk food   98\n",
       "99   Brazilian junk food   99\n",
       "100  Brazilian junk food  100\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim = \"Brazilian junk food\"\n",
    "claim = [claim] * len(df1.index)\n",
    "claims = pd.DataFrame(claim)\n",
    "claims['id'] = range(len(df1.index))\n",
    "claims.to_csv('claims.csv')\n",
    "# claim = np.array(claim)\n",
    "# # print type(claim)\n",
    "# claims = df1.loc[:,['id']]\n",
    "# claims = pd.concat([claims,pd.DataFrame(claim)])\n",
    "# claims = pd.DataFrame(claim)\n",
    "# claims.columns = ['claim']\n",
    "# claims.to_csv('claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                        claim\n",
      "0    0  US coastal growth continues\n",
      "1    1  US coastal growth continues\n",
      "2    2  US coastal growth continues\n",
      "3    3  US coastal growth continues\n",
      "4    4  US coastal growth continues\n",
      "5    5  US coastal growth continues\n",
      "6    6  US coastal growth continues\n",
      "7    7  US coastal growth continues\n",
      "8    8  US coastal growth continues\n",
      "9    9  US coastal growth continues\n",
      "10  10  US coastal growth continues\n",
      "11  11  US coastal growth continues\n",
      "12  12  US coastal growth continues\n",
      "13  13  US coastal growth continues\n",
      "14  14  US coastal growth continues\n",
      "15  15  US coastal growth continues\n",
      "16  16  US coastal growth continues\n",
      "17  17  US coastal growth continues\n",
      "18  18  US coastal growth continues\n",
      "19  19  US coastal growth continues\n",
      "20  20  US coastal growth continues\n",
      "21  21  US coastal growth continues\n",
      "22  22  US coastal growth continues\n",
      "23  23  US coastal growth continues\n",
      "24  24  US coastal growth continues\n",
      "25  25  US coastal growth continues\n",
      "26  26  US coastal growth continues\n",
      "27  27  US coastal growth continues\n",
      "28  28  US coastal growth continues\n",
      "29  29  US coastal growth continues\n",
      "..  ..                          ...\n",
      "70  70  US coastal growth continues\n",
      "71  71  US coastal growth continues\n",
      "72  72  US coastal growth continues\n",
      "73  73  US coastal growth continues\n",
      "74  74  US coastal growth continues\n",
      "75  75  US coastal growth continues\n",
      "76  76  US coastal growth continues\n",
      "77  77  US coastal growth continues\n",
      "78  78  US coastal growth continues\n",
      "79  79  US coastal growth continues\n",
      "80  80  US coastal growth continues\n",
      "81  81  US coastal growth continues\n",
      "82  82  US coastal growth continues\n",
      "83  83  US coastal growth continues\n",
      "84  84  US coastal growth continues\n",
      "85  85  US coastal growth continues\n",
      "86  86  US coastal growth continues\n",
      "87  87  US coastal growth continues\n",
      "88  88  US coastal growth continues\n",
      "89  89  US coastal growth continues\n",
      "90  90  US coastal growth continues\n",
      "91  91  US coastal growth continues\n",
      "92  92  US coastal growth continues\n",
      "93  93  US coastal growth continues\n",
      "94  94  US coastal growth continues\n",
      "95  95  US coastal growth continues\n",
      "96  96  US coastal growth continues\n",
      "97  97  US coastal growth continues\n",
      "98  98  US coastal growth continues\n",
      "99  99  US coastal growth continues\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "claims2 = pd.read_csv('claims.csv')\n",
    "# claims2 = claims2.loc[:,'claim']\n",
    "claims2.columns = ['id','claim']\n",
    "# claims2.to_csv('claims.csv')\n",
    "print claims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>At least 25 people in one Florida county have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Northam said in an interview that \"an awak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida began the colossal task of cleaning up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The northern eyewall of Hurricane Irma has str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The largest evacuation in US history is underw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>MIAMI (AP) - The Latest on Hurricane Irma (all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Excerpts from recent editorials in the United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Miami was thrown a lifeline on Saturday as Hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>WASHINGTON (AP) - The Environmental Protection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Fire fighters are now going door-to-door in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>THE BIG IDEA: Last August, Donald Trump attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>LOWER MATECUMBE KEY, Fla. -- The Latest on Hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>ST. PETERSBURG, Fla. -- The Latest on Hurrican...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>MIAMI -- The Latest on Hurricane Irma (all tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>MIAMI -- The Latest on Hurricane Irma (all tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>TALLAHASSEE, Fla. -- The Latest on Irma (all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Skip in Skipx\\n\\nEmbed\\n\\nx\\n\\nShare\\n\\nCLOSE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Skip in Skipx\\n\\nEmbed\\n\\nx\\n\\nShare\\n\\nCLOSE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Congress has once again tried and failed to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Grassroots progressives are rising up and figh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>A weakened Irma took its parting shot at Flori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Tropical Storm Irma batters Florida, heads nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>MIAMI (AP) \" The Latest on Hurricane Irma (all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>Sponsored content is written by Global News' w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>The scope and scale of misery caused by Irma c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>MIAMI - Florida residents picked store shelves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Excerpts from recent editorials in the United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>Sponsored content is written by Global News' w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>As Hurricane Irma continues toward the mainlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>Inmates on death row in Arizona Fullscreen Ari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>Irma's rampage across Florida broke records in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>Residents along the Florida Panhandle are bein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>HARTFORD, Conn., Sep 08, 2017 (BUSINESS WIRE) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>COLUMBIA, S.C. (AP) -- The cleanup effort cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>COLUMBIA, S.C. (AP) -- The Latest on the effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>COLUMBIA, S.C. (AP) -- The cleanup effort cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>MIAMI (AP) -- The Latest on Hurricane Irma (al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>There's a wild bunch riding out Hurricane Irma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>COLUMBIA, S.C. (AP) -- The Latest on the effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>&lt;iframe width=\"476\" height=\"267\" src=\"http://a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>MIAMI (AP) -- The Latest on Hurricane Irma (al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>ST. PETERSBURG, Fla. (AP) -- The Latest on Hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>TALLAHASSEE, Fla. (AP) -- The Latest on Irma (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>HOUSTON (AP) -- The Latest on Tropical Storm H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>MIAMI (AP) -- The Latest on Hurricane Irma (al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>Rising sea levels and fierce storms have faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>ST. PETERSBURG, Fla. (AP) -- The Latest on Hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>HOUSTON: Authorities say they have confirmed s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>At the beginning of August, the Bureau of Labo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>ST. PETERSBURG, Fla. (AP) _ The Latest on Hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>Good News for the Corn Crop - Tracking Jose\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>MS. SANDERS: Good afternoon, everybody. I hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>The cleanup effort continues in South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>HOUSTON (AP) _ The Latest on Tropical Depressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>Heavy rainfall and flooding is expected in muc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id                                               text\n",
       "0             0    0  At least 25 people in one Florida county have ...\n",
       "1             1    1  Mr. Northam said in an interview that \"an awak...\n",
       "2             2    2  Florida began the colossal task of cleaning up...\n",
       "3             3    3  The northern eyewall of Hurricane Irma has str...\n",
       "4             4    4  Rising sea levels and fierce storms have faile...\n",
       "5             5    5  The largest evacuation in US history is underw...\n",
       "6             6    6  MIAMI (AP) - The Latest on Hurricane Irma (all...\n",
       "7             7    7  Excerpts from recent editorials in the United ...\n",
       "8             8    8  Miami was thrown a lifeline on Saturday as Hur...\n",
       "9             9    9  WASHINGTON (AP) - The Environmental Protection...\n",
       "10           10   10  Fire fighters are now going door-to-door in a ...\n",
       "11           11   11  THE BIG IDEA: Last August, Donald Trump attack...\n",
       "12           12   12  LOWER MATECUMBE KEY, Fla. -- The Latest on Hur...\n",
       "13           13   13  ST. PETERSBURG, Fla. -- The Latest on Hurrican...\n",
       "14           14   14  MIAMI -- The Latest on Hurricane Irma (all tim...\n",
       "15           15   15  MIAMI -- The Latest on Hurricane Irma (all tim...\n",
       "16           16   16  TALLAHASSEE, Fla. -- The Latest on Irma (all t...\n",
       "17           17   17  Skip in Skipx\\n\\nEmbed\\n\\nx\\n\\nShare\\n\\nCLOSE\\...\n",
       "18           18   18  Skip in Skipx\\n\\nEmbed\\n\\nx\\n\\nShare\\n\\nCLOSE\\...\n",
       "19           19   19  Congress has once again tried and failed to re...\n",
       "20           20   20  Grassroots progressives are rising up and figh...\n",
       "21           21   21  A weakened Irma took its parting shot at Flori...\n",
       "22           22   22  Tropical Storm Irma batters Florida, heads nor...\n",
       "23           23   23  MIAMI (AP) \" The Latest on Hurricane Irma (all...\n",
       "24           24   24  Sponsored content is written by Global News' w...\n",
       "25           25   25  The scope and scale of misery caused by Irma c...\n",
       "26           26   26  MIAMI - Florida residents picked store shelves...\n",
       "27           27   27  Excerpts from recent editorials in the United ...\n",
       "28           28   28  Sponsored content is written by Global News' w...\n",
       "29           29   29  As Hurricane Irma continues toward the mainlan...\n",
       "..          ...  ...                                                ...\n",
       "71           71   71  Inmates on death row in Arizona Fullscreen Ari...\n",
       "72           72   72  Irma's rampage across Florida broke records in...\n",
       "73           73   73  Rising sea levels and fierce storms have faile...\n",
       "74           74   74  Residents along the Florida Panhandle are bein...\n",
       "75           75   75  HARTFORD, Conn., Sep 08, 2017 (BUSINESS WIRE) ...\n",
       "76           76   76  COLUMBIA, S.C. (AP) -- The cleanup effort cont...\n",
       "77           77   77  COLUMBIA, S.C. (AP) -- The Latest on the effec...\n",
       "78           78   78  Rising sea levels and fierce storms have faile...\n",
       "79           79   79  COLUMBIA, S.C. (AP) -- The cleanup effort cont...\n",
       "80           80   80  MIAMI (AP) -- The Latest on Hurricane Irma (al...\n",
       "81           81   81  There's a wild bunch riding out Hurricane Irma...\n",
       "82           82   82  Rising sea levels and fierce storms have faile...\n",
       "83           83   83  COLUMBIA, S.C. (AP) -- The Latest on the effec...\n",
       "84           84   84  <iframe width=\"476\" height=\"267\" src=\"http://a...\n",
       "85           85   85  MIAMI (AP) -- The Latest on Hurricane Irma (al...\n",
       "86           86   86  Rising sea levels and fierce storms have faile...\n",
       "87           87   87  ST. PETERSBURG, Fla. (AP) -- The Latest on Hur...\n",
       "88           88   88  TALLAHASSEE, Fla. (AP) -- The Latest on Irma (...\n",
       "89           89   89  HOUSTON (AP) -- The Latest on Tropical Storm H...\n",
       "90           90   90  MIAMI (AP) -- The Latest on Hurricane Irma (al...\n",
       "91           91   91  Rising sea levels and fierce storms have faile...\n",
       "92           92   92  ST. PETERSBURG, Fla. (AP) -- The Latest on Hur...\n",
       "93           93   93  HOUSTON: Authorities say they have confirmed s...\n",
       "94           94   94  At the beginning of August, the Bureau of Labo...\n",
       "95           95   95  ST. PETERSBURG, Fla. (AP) _ The Latest on Hurr...\n",
       "96           96   96  Good News for the Corn Crop - Tracking Jose\\n\\...\n",
       "97           97   97  MS. SANDERS: Good afternoon, everybody. I hope...\n",
       "98           98   98  The cleanup effort continues in South Carolina...\n",
       "99           99   99  HOUSTON (AP) _ The Latest on Tropical Depressi...\n",
       "100         100  100  Heavy rainfall and flooding is expected in muc...\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv('bodies.csv')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "try\n",
      "finally\n",
      "try\n",
      "finally\n",
      "try\n",
      "finally\n",
      "try\n",
      "finally[['afghanistan', 'trump', 'zone', 'kabul', 'american', 'embassy', 'green', 'digging', 'mr', 'security', 'afghan', 'expands', 'decade'], ['deir', 'city', 'group', 'isis', 'syrian', 'coalition', 'strike', 'usbacked', 'rebel', 'forces', 'ezzor', 'russian', 'syria', 'sdf'], ['shown', 'death', 'chicago', 'footage', 'hotel', 'freezer', 'surveillance', 'jenkins', 'video', 'martin', 'cnn', 'seen', 'told'], ['selfsustaining', 'school', 'returning', 'keys', 'west', 'florida', 'county', 'officials', 'key', 'water', 'residents', 'south']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "\n",
    "mutex = Lock()\n",
    "\n",
    "aggregate_keywords = []\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, url):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.url = url\n",
    "\n",
    "    def run(self):\n",
    "        get_keywords(self.url)\n",
    "        \n",
    "def get_keywords(user_url):\n",
    "    url = user_url.decode('utf-8')\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "\n",
    "    keywords = article.keywords\n",
    "    kl = []\n",
    "    for word in keywords:\n",
    "        kl.append(word.encode('utf-8'))\n",
    "    mutex.acquire()\n",
    "    try:\n",
    "        aggregate_keywords.append(kl)\n",
    "        print 'try'\n",
    "    finally:\n",
    "        mutex.release()\n",
    "        print 'finally'\n",
    "\n",
    "urls = ['https://www.nytimes.com/2017/09/16/world/asia/kabul-green-zone-afghanistan.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news',\n",
    "        'http://abcnews.go.com/US/wireStory/irma-shuttered-schools-add-stress-families-49894226?cid=clicksource_4380645_1_hero_headlines_headlines_hed',\n",
    "       'http://www.cnn.com/2017/09/16/us/chicago-freezer-death/index.html',\n",
    "       'http://www.cnn.com/2017/09/16/politics/russia-fires-on-us-backed-forces/index.html']\n",
    "\n",
    "index = 0\n",
    "threads = []\n",
    "for url in urls: \n",
    "    threads.append(myThread(url))\n",
    "    threads[index].start()\n",
    "    index += 1\n",
    "    print index\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print aggregate_keywords\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io, json\n",
    "dataframe = pd.read_csv('articles.csv')\n",
    "\n",
    "dataframe = dataframe.loc[:100,['source','text','url']]\n",
    "dataframe.to_csv('100_articles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
